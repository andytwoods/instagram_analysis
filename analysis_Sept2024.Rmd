---
title: "instagram_data_combiner"
author: "Andy Woods"
date: "2024-06-07"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(corrr)
library(patchwork)
library(kableExtra)
# library(MASS)
# library(ggeffects)
# library(effects)
if(!require(psych)){install.packages("psych")}
# if(!require(FSA)){install.packages("FSA")}
# if(!require(lattice)){install.packages("lattice")}
# if(!require(ordinal)){install.packages("ordinal")}
# if(!require(car)){install.packages("car")}
# if(!require(RVAideMemoire)){install.packages("RVAideMemoire")}
# if(!require(multcomp)){install.packages("multcomp")}
if(!require(emmeans)){install.packages("emmeans")}
# if(!require(viridis)){install.packages("viridis")}
library(ggplot2)
# library(reshape2)
# library(RColorBrewer)
# library(ggthemes)
# library(brms)
# library("ggdist")
library(viridis)
library(gtsummary)
# library(patchwork)
library(sjPlot)
# library(multcomp)
library(lme4)
# library(magrittr)
# library(purrr)
# library(forcats)
# library(tidyr)
# library(modelr)
# library(ggdist)
# # library(tidybayes)
# library(ggplot2)
# library(cowplot)
# library(ggrepel)
# library(RColorBrewer)
# # library(gganimate)
# library(posterior)
# library(distributional)
library(robustlmm)
library(performance)
library(corrr)

```

control data csv
```{r}
control_data <- tibble(read.csv("instagram_control_n100.csv"))

```



load csv
```{r}

replace_many_names <- function(ds1, ds2, from_nam, to_nam){
      colnames(ds1) <- sub(from_nam, to_nam, colnames(ds1))
}

load_data_per_cohort <-
  function(t0_file_nam, tx_file_nam, cohort_name) {
    
    clean_duplicates <- function(tib, prefix){
      outcome <- tib |>
        mutate(unique_sj_id = if_else(str_length(prolific_id) == 24 , 
                                   paste0(prefix, '_', cohort_name, '_unique_id_', prolific_id), prolific_id, 
                                   paste0(prefix, '_', cohort_name, '_unique_id_', row_number()
                                          ))) |>
        distinct(unique_sj_id, .keep_all=T) |>
        filter(str_length(prolific_id) < 28) |>
        mutate_all(as.character)
      # print(paste0(cohort_name,'  ', prefix,  ' before cleaned:', nrow(outcome), '   after cleaned:', nrow(tib)))
      
      # [1] "mil  to before cleaned:567   after cleaned:681"
      # [1] "mil  tx before cleaned:390   after cleaned:407"
      # [1] "map  to before cleaned:572   after cleaned:715"
      # [1] "map  tx before cleaned:385   after cleaned:400"
      # [1] "bbc  to before cleaned:844   after cleaned:1055"
      # [1] "bbc  tx before cleaned:370   after cleaned:392"
      # [1] "va  to before cleaned:591   after cleaned:704"
      # [1] "va  tx before cleaned:366   after cleaned:390"
      return(outcome)
    }

    
    t0_df = clean_duplicates(tibble(read.csv(t0_file_nam)), "to")
    tx_df = clean_duplicates(tibble(read.csv(tx_file_nam)), "tx")
    
    rename_cols = function(from_list, to_list){
      for(i in 1:length(from_list)){
        colnames(t0_df) <- sub(from_list[i], to_list[i], colnames(t0_df))
      }
      # to update the dataframe outside of the scope of this function we need to use the below odd notation <<-
      t0_df <<- t0_df
    }
        # correcting wrongly titled columns
    if (cohort_name == "va") {
      t0_df |>
        rename(Q_dre_2 = question3,
               Q_dre_3 = question4,)
      tx_df |>
        rename(Q_dre_2 = question3,
               Q_dre_3 = question4,)
      rename_cols(c('wearable', 'theatrical', 'v.a', 'dress', '.shrinking'), c('tia', 'the', 'va', 'dre', 'shr'))
    }
    
    else if(cohort_name == 'mil'){
      rename_cols(c('skull', 'science', 'satellite', 'music', 'victorian'), c('sku', 'met', 'spa', 'mus', 'vic'))
    }
    
    else if(cohort_name == 'map'){
      rename_cols(c('artify', 'jyoti', 'shadow', 'stories', 'rider'), c('ay', 'dd', 'sp', 'ss', 'el'))
    }
  
    else if(cohort_name == 'bbc'){
      rename_cols(c('restore', 'feed', 'meerkat', 'life', 'creatures'), c('res', 'hum', 'ani', 'lif', 'dee'))
    }

    # below, note how presence_beingthere has NO _ suffix. On purpose
    t0 <- t0_df |> dplyr::select(starts_with("Q_"), user_at_start, age, ar_frequency, duration_seconds,
                          study_name, ar_frequency, prolific_id, created, username,
                          starts_with("enjoy_"),  starts_with("knew_"), starts_with("surround_"), starts_with("captivated_"),
                          starts_with("presence_beingthere"),  starts_with("emotion_awe_"),  starts_with("emotion_boredom_"),
                          starts_with("presence_real_") )

    tx <- tx_df |>
    dplyr::select(starts_with("Q_"), starts_with("memory_score_"), prolific_id, created, unique_sj_id, ) |>
      rename_with( ~ paste0("X1monthlater_", .x),
                   starts_with("Q_")) |>
      # below, removing 'created' via
      mutate(X1monthlater_created = created, .keep = "unused",  .before=0)
    
    
    combined <- full_join(tx, t0, by = "prolific_id") |> 
      relocate(
        X1monthlater_created, created, .before = NULL,
      )
    
    
    
    combined <- combined |>
      mutate(
        date_of_test2 = parse_datetime(X1monthlater_created),
        date_of_test1 = parse_datetime(created),
        days =  as.integer(difftime(date_of_test2, date_of_test1)),
        no_followup=is.na(days),
        days = if_else(is.na(days), 0, days),
        media = case_when(
          str_detect(study_name, '_FLAT') ~ 'flat',
          str_detect(study_name, '_AR') ~ 'ar',
        ),
        .before = 1
      ) |>
      dplyr::select(-created,-X1monthlater_created)

    combined <- combined |>
      mutate(id = row_number(),
             cohort = cohort_name, ) 

    combined <- combined |>
        pivot_longer(cols=contains("Q_"),
               values_to='score',
               names_to='exp_title_question') 
    
    combined <- combined |>
      mutate(
        from_x_months_later_dataset = exp_title_question,
        exp_title_question = str_remove(exp_title_question, "X1monthlater_Q_"),
        exp_title_question = str_remove(exp_title_question, "Q_"),
        exp_title = gsub("_", "", str_extract(exp_title_question, ".*_")),) 
    
    #####################

    for(colname in c("enjoy_", "knew_", "surround_", "captivated_", "presence_beingthere_", "emotion_awe_", "emotion_boredom_", "presence_real_")) {
      
      cleaned_colname = substr(colname, 1, nchar(colname)-1)
      
      combined_length = nrow(combined)
      combined[, cleaned_colname] = NA
      for(i in 1:combined_length){
        lookup_col = paste0(colname, combined$exp_title[i])
        retrieved = combined[i, lookup_col]
        combined[i, cleaned_colname] <- retrieved
      }
    }

    #####################  
    
    
    combined <- combined |>
      mutate(
        gap = if_else(days <50, 0, 1),
        days = if_else(str_detect(from_x_months_later_dataset, "X1monthlater"), days, 0),
        .before = 1
        ) |>
      filter(user_at_start=="AnonymousUser") |>
      dplyr::select(date_of_test2, date_of_test1, days, media, duration_seconds, unique_sj_id,  age, ar_frequency,  study_name, cohort, exp_title_question, score, exp_title, enjoy, 
                    knew, surround, captivated, presence_beingthere, emotion_awe, emotion_boredom, presence_real, gap, no_followup)
      
    return(combined)
  }


mil <- load_data_per_cohort("data_compile/mil2024.csv", "data_compile/mil_9month_FINAL.csv", 'mil')
map <- load_data_per_cohort("data_compile/map2024.csv", "data_compile/map_9month_FINAL.csv", 'map')
bbc <- load_data_per_cohort("data_compile/bbc2024.csv", "data_compile/bbc_9month_FINAL.csv", 'bbc')
va <- load_data_per_cohort("data_compile/va2024.csv","data_compile/va_9month_FINAL.csv", 'va')


combined_data <- bind_rows(mil, map, bbc, va) |>
  rename(id = unique_sj_id) |>
    mutate(
     ar_frequency = case_when(
       ar_frequency == "Never" ~ 0,
       ar_frequency == "Sometimes, but less than once a month" ~ 1,
       ar_frequency == "About once a month" ~ 2,
       ar_frequency == "Several times a month" ~ 3,
       ar_frequency == "A few times a week" ~ 4,
       ar_frequency == "About daily" ~ 5
       ),
     id = factor(id),
     score = case_when(
       score == 1 ~ 1,
       score < 1 ~ 0,
       TRUE ~ NA),
     media = factor(media),
     exp_title_question = str_remove(exp_title_question,"X1monthlater_"),
     exp_title = gsub("_", "", str_extract(exp_title_question, ".*_")),
     cohort = factor(cohort),
     presence_real = as.integer(presence_real),
     emotion_awe = as.integer(emotion_awe),
     emotion_boredom = as.integer(emotion_boredom),
     presence_beingthere = as.integer(presence_beingthere),
     captivated = as.integer(captivated),
     surround = as.integer(surround),
     knew = suppressWarnings(as.integer(knew)), # we have some NAs here
     months = days/30,
     months=case_when(
          days == 0 ~ "t0",
          days < 50 ~ "t1",
          days < 400 ~ "t2"
        ),
     months=factor(months),
     # nb someone entered a stupidly high age, so we set that to NA below
     # note that age is in decades
     age = as.integer(age),
     age = if_else(age < 100, age/10, NA)) |>
    filter(exp_title_question!="dd_1.Comment") 


saveRDS(mil, 'mil.rds')
saveRDS(map, 'map.rds') 
saveRDS(bbc, 'bbc.rds') 
saveRDS(va, 'va.rds') 
saveRDS(combined_data, "combined_data.rds")

```


```{r}
combined_data <- readRDS("combined_data.rds")
```



# compiling demographics
```{r}
combined_data |>
  dplyr::summarize(
                   SJs=n_distinct(id), 
                   age_mean=mean(age*10, na.rm=T), 
                   age_sd=sd(age*10, na.rm=T),
                   age_min=min(age*10, na.rm=T),
                   age_max=max(age*10, na.rm=T), 
                   ) |> print(n=24)

#     SJs age_mean age_sd age_min age_max
#   <int>    <dbl>  <dbl>   <dbl>   <dbl>
# 1314	28.1621	9.395616	18	81



combined_data |>
    group_by(cohort, months, media) |>
  dplyr::summarize(
                   SJs=n_distinct(id), 
                   age_mean=sprintf("%.1f (%.1f)", mean(age*10, , na.rm=T) , sd(age*10), , na.rm=T), na.rm = TRUE
                   ) |> 
  kable("html", 
        col.names = c("cohort", "months", "media", "count", "age (sd)")
        ) |>
  collapse_rows(columns = 1:3, valign = "top") |>
  kable_styling()


#    cohort delay media mean1 datapoints   SJs
#    <fct>  <chr> <fct> <dbl>      <int> <int>
#  1 bbc    0     ar    0.610       2284   157
#  2 bbc    0     flat  0.681       2221   149
#  3 bbc    1     ar    0.532       1213    82
#  4 bbc    1     flat  0.553       1218    82
#  5 bbc    2     ar    0.457       1158    78
#  6 bbc    2     flat  0.494       1064    72
#  7 map    0     ar    0.432       2320   164
#  8 map    0     flat  0.575       2408   165
#  9 map    1     ar    0.383       1184    80
# 10 map    1     flat  0.447       1231    83
# 11 map    2     ar    0.365       1249    84
# 12 map    2     flat  0.330       1211    82
# 13 mil    0     ar    0.503       2443   166
# 14 mil    0     flat  0.572       2521   171
# 15 mil    1     ar    0.454       1155    79
# 16 mil    1     flat  0.498       1219    83
# 17 mil    2     ar    0.465       1300    88
# 18 mil    2     flat  0.454       1296    88
# 19 va     0     ar    0.414       2051   160
# 20 va     0     flat  0.576       1857   144
# 21 va     1     ar    0.365       1084    84
# 22 va     1     flat  0.434        963    75
# 23 va     2     ar    0.341       1002    78
# 24 va     2     flat  0.349        898    70

combined_data |>
  dplyr::select(gender_1) |>
  tbl_summary(statistic = list(
      all_continuous() ~ "{mean} ({sd})",
      all_categorical() ~ "{n} / {N} ({p}%)"
    ))

combined_data |>
  dplyr::select(country) |>
  tbl_summary(sort = all_categorical() ~ "frequency", statistic = list(
      all_continuous() ~ "{mean} ({sd})",
      all_categorical() ~ "{n} / {N} ({p}%)"
    ), missing='always')

```

# checking data validity
```{r}
# wise to eyeball data to make sure expected #SJs (allowance for voluntary entry of data => missing data)
combined_data |>
    group_by(cohort, months, media) |>
  dplyr::summarize(mean1 = mean(score),
                   datapoints= n(), 
                   SJs=n_distinct(id), 
                   age_mean=mean(age*10), 
                   age_sd=sd(age*10)
                   ) |> print(n=24)
#    cohort delay media mean1 datapoints   SJs
#    <fct>  <chr> <fct> <dbl>      <int> <int>
#  1 bbc    0     ar    0.610       2284   157
#  2 bbc    0     flat  0.681       2221   149
#  3 bbc    1     ar    0.532       1213    82
#  4 bbc    1     flat  0.553       1218    82
#  5 bbc    2     ar    0.457       1158    78
#  6 bbc    2     flat  0.494       1064    72
#  7 map    0     ar    0.432       2320   164
#  8 map    0     flat  0.575       2408   165
#  9 map    1     ar    0.383       1184    80
# 10 map    1     flat  0.447       1231    83
# 11 map    2     ar    0.365       1249    84
# 12 map    2     flat  0.330       1211    82
# 13 mil    0     ar    0.503       2443   166
# 14 mil    0     flat  0.572       2521   171
# 15 mil    1     ar    0.454       1155    79
# 16 mil    1     flat  0.498       1219    83
# 17 mil    2     ar    0.465       1300    88
# 18 mil    2     flat  0.454       1296    88
# 19 va     0     ar    0.414       2051   160
# 20 va     0     flat  0.576       1857   144
# 21 va     1     ar    0.365       1084    84
# 22 va     1     flat  0.434        963    75
# 23 va     2     ar    0.341       1002    78
# 24 va     2     flat  0.349        898    70


# below shows that we indeed have 5 non-overlapping experiences per cohort
combined_data |>
  ggplot(aes(x=months, y=score, group=media)) +
    stat_summary(fun=mean, colour="red", geom="line",group=1) +
  facet_grid(rows=vars(exp_title), cols=vars(cohort))


# below shows that in# below shows that in# below shows that indeed months*media pattern common over all experiences
# helping check no systematic oddities in data
combined_data |>
  mutate(
    exp_title = case_when(
      exp_title %in% c("ani", "ay", "met", "dre") ~ 'a',
      exp_title %in% c("dee", "dd", "mus", "shr") ~ 'b',
      exp_title %in% c("hum", "el", "sku", "the") ~ 'c',
      exp_title %in% c("lif", "sp", "spa", "tia") ~ 'd',
      exp_title %in% c("res", "ss", "vic", "va") ~ 'e',
    )
  ) |>
  ggplot(aes(x=months, y=score, group=media, color=media)) +
  stat_summary(fun=mean, geom="line") +
  facet_grid(rows=vars(exp_title), cols=vars(cohort))


combined_data |>
  corrr::correlate() |>
  corrr::rearrange(method = "MDS", absolute = FALSE) %>%
  corrr::shave() %>% 
  corrr::rplot(shape = 19, legend = TRUE, print_cor=T)
# captivated highly correlates with emotion_aws(.67), presence_real(.64), surround (.74) and presenec_beingthere (.73)
# so excluding from analyses (no real hypotheses from this)

combined_data |>
  select(-captivated) |>
  corrr::correlate() |>
  corrr::rearrange(method = "MDS", absolute = FALSE) %>%
  corrr::shave() %>% 
  corrr::rplot(shape = 19, legend = TRUE, print_cor=T)
# presence_real and presence_beingthere highly correlated (.72)
# presence_real and surround highly correlated (.66)
# presence_beingthere and surround highly correlated (77)
# so averaged into new presence measure

combined_data |>
  rowwise() |>
  mutate(presence=mean(c_across(c(presence_beingthere, presence_real, surround)))) |>
  select(-captivated, -presence_beingthere, -presence_real, -surround) |>
  corrr::correlate() |>
  corrr::rearrange(method = "MDS", absolute = FALSE) %>%
  corrr::shave() %>% 
  corrr::rplot(shape = 19, legend = TRUE, print_cor=T)
# presence_real and presence_beingthere highly correlated (.72) so averaged into new presence measure
# removing surround as highly correlates with others


combined_data_cor_screened <- combined_data |>
  rowwise() |>
  mutate(presence=mean(c_across(c(presence_beingthere, presence_real, surround)))) |>
  select(-captivated, -presence_beingthere, -presence_real)
  
# presence_real and presence_beingthere highly correlated (.72) so averaged into new presence measure


# to help with convergence issues, we center and scale continuous variables
# https://stackoverflow.com/a/60835439/960471

combined_data_cor_screened <- combined_data_cor_screened |>
  mutate(
    enjoy = as.integer(enjoy),
    age = age, # x10 as above we divide by 10. But scale does the same thing
    #presence = scale(presence),
    #knew = scale(knew),
    #ar_frequency = scale(ar_frequency)
  )
  

# checking we have approp number of conditions
g <- plot_model(fit.model, type = "pred", terms = c("exp_title", "exp_title_question"), pred.type='re', ci.lvl=NA, colors = 'darkgray', show.legend = T, dot.size=3, dodge = 0, alpha=.5)  + theme_minimal() 
g


```



checking assumptions
```{r}

tab_model(fit.model)
performance::check_model(fit.model) 

binned_residuals_outcome <- performance::binned_residuals(fit.model) 
# Warning: Probably bad model fit. Only about 45% of the residuals are inside the error bounds.
# AW: implies predictor log transform maybe needed https://easystats.github.io/performance/reference/binned_residuals.html
plot(binned_residuals_outcome, show_dots = TRUE)

# note that there is argument for more theoretical approach (https://stats.stackexchange.com/questions/99274/interpreting-a-binned-residual-plot-in-logistic-regression) to binned residuals here https://cran.r-project.org/web/packages/DHARMa/vignettes/DHARMa.html
```

# checking for v poor performing and v high performing questions
# outcome: only a 4 <.25 and 4ish > .75. Perhaps not that impactful
```{r}
combined_data_cor_screened |>
  group_by(media, exp_title, exp_title_question) |>
  dplyr::summarize(mean_over_time = mean(score),
                   datapoints= n(), 
                   SJs=n_distinct(id), 
                   ) |> 
  filter(mean_over_time <= .25 | mean_over_time >= .75) |>
  #arrange(desc(mean_over_time)) |>
  ggplot(aes(x=reorder(exp_title_question, mean_over_time),y=mean_over_time, fill=media)) + geom_bar(stat="identity", position=position_dodge()) 
```





# Combining advice from https://stats.stackexchange.com/, as well as some more predictors

https://stats.stackexchange.com/questions/653787/model-specification-for-causal-inference-with-longitudinal-data-and-multilevel-m/654298#654298

Why are we using binary regression and not CLM? We were advised:
'The simplest case is to treat correctness of an answer to each question as a binary correct, forming 15 rows for each id in a wave of study. This is appropriate if selection of the other two options offers no additional information. This approach requires a mixed-effect binary logit model. See lme4::glmer() and glmmTMB::glmmTMB().'
AW: I have gone with above approach as indeed, two other options offer no additional information

Why is there (1| exp_title : exp_title_question) in fit.followup? This is not specified in stats.stackexchange.com
AW: We are using the 'simplest case' (see prev answer). We have not collapsed exp_title_question and so need to account for it


```{r}

##### one analalysed with gap

combined_new <- combined_data_cor_screened |>
  mutate(
    time = case_when(
          months == "t0" ~ "t0",
          months == "t1" ~ "t1",
          months == "t2" ~ "t1"
        ),
    time = factor(time),
    gap = factor(gap, levels=c(0,1), labels=c("1m", "6m")),
    no_followup = factor(no_followup)
  ) 

combined_new |>
  filter(days==0) |>
  group_by(media, exp_title, exp_title_question, no_followup) |>
  dplyr::summarize(
                   datapoints= n(), 
                   )

fit.initial <- combined_new |>
  filter(days==0) |>
                glmer(formula = score ~ 1 + age + ar_frequency + presence + knew + media * cohort * no_followup
                     + (1| exp_title : exp_title_question),  
                      family = binomial("logit"), nAGQ=0, control=glmerControl(optimizer = "nloptwrap"))
# note removed  +  (1  | cohort : exp_title) + (1 | cohort : id)  as did not converge



emms <- emmeans(fit.initial, ~ media * cohort * no_followup)
contrast_results <- contrast(emms, interaction = "pairwise", by = c("media", "cohort"))
# media = ar, cohort = bbc:
#  no_followup_pairwise estimate     SE  df z.ratio p.value
#  FALSE - TRUE          0.17529 0.0578 Inf   3.033  0.0024
# 
# media = flat, cohort = bbc:
#  no_followup_pairwise estimate     SE  df z.ratio p.value
#  FALSE - TRUE          0.00941 0.0788 Inf   0.119  0.9049
# 
# media = ar, cohort = map:
#  no_followup_pairwise estimate     SE  df z.ratio p.value
#  FALSE - TRUE          0.01879 0.0772 Inf   0.244  0.8076
# 
# media = flat, cohort = map:
#  no_followup_pairwise estimate     SE  df z.ratio p.value
#  FALSE - TRUE          0.22126 0.0747 Inf   2.961  0.0031
# 
# media = ar, cohort = mil:
#  no_followup_pairwise estimate     SE  df z.ratio p.value
#  FALSE - TRUE         -0.00361 0.0752 Inf  -0.048  0.9617
# 
# media = flat, cohort = mil:
#  no_followup_pairwise estimate     SE  df z.ratio p.value
#  FALSE - TRUE          0.04011 0.0748 Inf   0.536  0.5918
# 
# media = ar, cohort = va:
#  no_followup_pairwise estimate     SE  df z.ratio p.value
#  FALSE - TRUE          0.04660 0.0786 Inf   0.593  0.5532
# 
# media = flat, cohort = va:
#  no_followup_pairwise estimate     SE  df z.ratio p.value
#  FALSE - TRUE          0.03701 0.0756 Inf   0.489  0.6246
# 
# Results are given on the log odds ratio (not the response) scale. 

plot_model(fit.initial, type='int')[[4]]
# note how qualitatively we have nearly identical pattern of results. We have no reason to suspect  


```


```{r}


fit.followup <- glmer(formula = score ~ 1 + age + ar_frequency + presence + knew + media * time * gap * cohort 
                     +  (1  | cohort : exp_title) + (1 | cohort : id) + (1| exp_title : exp_title_question),  
                     family = binomial("logit"), nAGQ=0, control=glmerControl(optimizer = "nloptwrap"), data=combined_new)

emm <- emmeans(fit.mod1, ~media*gap*time, type='response' )

contrasts_list <- list(
  "At 9 months, media differs"         = c(0, 0, -1, 1, 0, 0, 0, 0),  # Compare media at time1 + gapB
  "At 3 months, media differs"         = c(-1, 1, 0, 0, 0, 0, 0, 0),  # Compare media at time1 + gapA
  
    "CHECK_AR_time0_gapA_vs_gapB"            = c(0, 0, 0, 0, 1, 0, -1, 0),  # Compare AR at time0 + gapA vs. time0 + gapB
  "CHECK_Video_time0_gapA_vs_gapB"         = c(0, 0, 0, 0, 0, 1, 0, -1),   # Compare Video at time0 + gapA vs. time0 + gapB
  "At 0 months, media differs"              = c(0, 0, 0, 0, -1, 1, -1, 1), # Compare media at time0 (across gapA and gapB)

   "AR_time0_vs_time1_gapA"           = c(-1, 0, 0, 0, 1, 0, 0, 0),  # Compare AR at time0 + gapA vs. AR at time1 + gapA
  "AR_time1_gapA_vs_time1_gapB"      = c(1, 0, -1, 0, 0, 0, 0, 0),  # Compare AR at time1 + gapA vs. AR at time1 + gapB
  "Video_time0_vs_time1_gapA"        = c(0, -1, 0, 0, 0, 1, 0, 0),  # Compare Video at time0 + gapA vs. Video at time1 + gapA
  "Video_time1_gapA_vs_time1_gapB"   = c(0, 1, 0, -1, 0, 0, 0, 0)  # Compare Video at time1 + gapA vs. Video at time1 + gapB

)
my_contrasts <- contrast(emm, contrasts_list)
summary(my_contrasts, adjust = "sidak")

 # contrast                       odds.ratio     SE  df null z.ratio p.value
 # At 9 months, media differs          1.006 0.0674 Inf    1   0.089  1.0000
 # At 3 months, media differs          1.245 0.0823 Inf    1   3.319  0.0081
 # CHECK_AR_time0_gapA_vs_gapB         0.993 0.0660 Inf    1  -0.103  1.0000
 # CHECK_Video_time0_gapA_vs_gapB      1.014 0.0675 Inf    1   0.203  1.0000
 # At 0 months, media differs          2.747 0.2593 Inf    1  10.700  <.0001
 # AR_time0_vs_time1_gapA              1.317 0.0648 Inf    1   5.594  <.0001
 # AR_time1_gapA_vs_time1_gapB         1.135 0.0757 Inf    1   1.895  0.4162
 # Video_time0_vs_time1_gapA           1.770 0.0845 Inf    1  11.969  <.0001
 # Video_time1_gapA_vs_time1_gapB      1.405 0.0930 Inf    1   5.133  <.0001
# P value adjustment: sidak method for 9 tests 

  # see how odds.ratio and z.ratio below is identical to above (except for combo tests above)
emmeans(fit.mod1, pairwise~media*gap*time, type='response', adjust = "none")

 # contrast                odds.ratio     SE  df null z.ratio p.value
 # ar 1m t1 / flat 1m t1        0.803 0.0531 Inf    1  -3.319  0.0009 ***********IMPORTANT
 # ar 1m t1 / ar 6m t1          1.135 0.0757 Inf    1   1.895  0.0580 ** AR IMPORTANT
 # ar 1m t1 / flat 6m t1        1.128 0.0759 Inf    1   1.791  0.0733
 # ar 1m t1 / ar 1m t0          0.759 0.0374 Inf    1  -5.594  <.0001 ** AR IMPORTANT
 # ar 1m t1 / flat 1m t0        0.454 0.0302 Inf    1 -11.889  <.0001
 # ar 1m t1 / ar 6m t0          0.754 0.0502 Inf    1  -4.241  <.0001
 # ar 1m t1 / flat 6m t0        0.460 0.0310 Inf    1 -11.531  <.0001
 # flat 1m t1 / ar 6m t1        1.413 0.0930 Inf    1   5.253  <.0001 ** FLAT IMPORTANT
 # flat 1m t1 / flat 6m t1      1.405 0.0930 Inf    1   5.133  <.0001
 # flat 1m t1 / ar 1m t0        0.946 0.0624 Inf    1  -0.845  0.3978
 # flat 1m t1 / flat 1m t0      0.565 0.0270 Inf    1 -11.969  <.0001 ** FLAT IMPORTANT
 # flat 1m t1 / ar 6m t0        0.939 0.0616 Inf    1  -0.955  0.3396
 # flat 1m t1 / flat 6m t0      0.573 0.0380 Inf    1  -8.409  <.0001
 # ar 6m t1 / flat 6m t1        0.994 0.0666 Inf    1  -0.089  0.9292 ***********IMPORTANT
 # ar 6m t1 / ar 1m t0          0.669 0.0446 Inf    1  -6.027  <.0001
 # ar 6m t1 / flat 1m t0        0.400 0.0265 Inf    1 -13.844  <.0001
 # ar 6m t1 / ar 6m t0          0.665 0.0323 Inf    1  -8.415  <.0001
 # ar 6m t1 / flat 6m t0        0.405 0.0272 Inf    1 -13.452  <.0001
 # flat 6m t1 / ar 1m t0        0.673 0.0452 Inf    1  -5.889  <.0001
 # flat 6m t1 / flat 1m t0      0.402 0.0268 Inf    1 -13.678  <.0001
 # flat 6m t1 / ar 6m t0        0.669 0.0447 Inf    1  -6.023  <.0001
 # flat 6m t1 / flat 6m t0      0.408 0.0201 Inf    1 -18.220  <.0001
 # ar 1m t0 / flat 1m t0        0.597 0.0397 Inf    1  -7.762  <.0001 ***********IMPORTANT
 # ar 1m t0 / ar 6m t0          0.993 0.0660 Inf    1  -0.103  0.9178
 # ar 1m t0 / flat 6m t0        0.605 0.0407 Inf    1  -7.459  <.0001
 # flat 1m t0 / ar 6m t0        1.663 0.1098 Inf    1   7.704  <.0001
 # flat 1m t0 / flat 6m t0      1.014 0.0675 Inf    1   0.203  0.8391 ***********IMPORTANT
 # ar 6m t0 / flat 6m t0        0.610 0.0408 Inf    1  -7.396  <.0001

```



plotting
```{r}

plot_model(fit.model, type = "pred", terms = c("months", "media"), ) #+ 
  #geom_line() + 
  #theme_minimal() + scale_x_continuous(aes(x=aw_mod(months)))
                   
                

plot <- combined_data_cor_screened |>
  mutate(months = str_remove(months, "t")) |>
  mutate(months = case_when(months=="0" ~ 0,
                            months=="1" ~ 1,
                            months=="2" ~ 7.5)) |>
  ggplot(aes(x=months, y=score, color=media)) +
 stat_summary(geom = "errorbar", fun.data = ~mean_se(., mult = 1.96), linewidth =1, width=.5) + 
    stat_summary(fun.y = mean, geom="line",  linewidth=1) +
  geom_rug(aes(y=score, x=days/30)) +
  annotate('ribbon', x = c(-Inf, Inf), ymin = .3134, ymax = .3966, 
           alpha = 0.4, fill = 'red') +
  geom_hline(yintercept=.355, linetype="dashed") + 
  theme(text=element_text(size=14))





plot |>
ggsave(filename='interaction.png', width=7.5, height=6.5, bg='white')



facet_plot <- combined_data_cor_screened |>
  mutate(
    cell_title = exp_title,
    exp_title = case_when(
      exp_title %in% c("res", "ay", "sku", "tia") ~ 'a',
      exp_title %in% c("hum", "dd", "met", "va") ~ 'b',
      exp_title %in% c("ani", "sp", "spa", "dre") ~ 'c',
      exp_title %in% c("lif", "ss", "mus", "shr") ~ 'd',
      exp_title %in% c("dee", "el", "vic", "the") ~ 'e',
    ),
    cohort = factor(cohort,  levels=c("mil","map","bbc", 'va'))
  ) |>
  ggplot(aes(x=months, y=score, group=media, color=media)) +
  stat_summary(fun=mean, geom="line") +
  geom_hline(yintercept=.25, linetype="dashed") + 
  facet_grid(rows=vars(exp_title), cols=vars(cohort)) +  theme(strip.text.y = element_blank())
facet_plot


titles <- c("res", "ay", "sku", "tia", "hum", "dd", "met", "va", "ani", "sp", "spa", "dre", "lif", "ss", "mus", "shr","dee", "el", "vic", "the")
  tag_facet(facet_plot, tag_pool = titles,
          open = "", close = "", y= 0.28, x = 1.75, hjust=0,
          size = 4,
          ) + theme(strip.text = element_text())



```


1-tail tests
```{r}
combined_data_cor_screened |>
  filter(months=="t2", media=="ar") |>
  select(score) |> 
  t.test(mu=0.35503, alternative = "greater")

# data:  select(filter(combined_data_cor_screened, months == "t2", media == "ar"), score)
# t = 7.9317, df = 4821, p-value = 1.333e-15
# alternative hypothesis: true mean is greater than 0.35503
# 95 percent confidence interval:
#  0.3995812       Inf
# sample estimates:
# mean of x 
# 0.4112401 

combined_data_cor_screened |>
  filter(months=="t2", media=="flat") |>
  select(score) |> 
  t.test(mu=0.35503, alternative = "greater")

# data:  select(filter(combined_data_cor_screened, months == "t2", media == "flat"), score)
# t = 7.2702, df = 4540, p-value = 2.102e-13
# alternative hypothesis: true mean is greater than 0.35503
# 95 percent confidence interval:
#  0.3960597       Inf
# sample estimates:
# mean of x 
# 0.4080599 

```


presence
```{r}


 combined_data_cor_screened_presence <- combined_data_cor_screened |>
  filter(days==0) |>
  mutate(exp_title_question = substr(exp_title_question, nchar(exp_title_question), nchar(exp_title_question))) |>
  filter(exp_title_question == 1) 

# note that we dropped the cohort random factor as the second model failed to converge
fit.presence <- combined_data_cor_screened_presence |> lmer(formula = presence ~ 1 + age + media + media * ar_frequency +  (1  | exp_title ) + (1 |  id), contrasts = list(media = contr.sum))

fit.presence_no_interaction <- combined_data_cor_screened_presence |> lmer(formula = presence ~ 1 + age + media + ar_frequency +  (1  | exp_title ) + (1 |  id), contrasts = list(media = contr.sum))

anova(fit.presence, fit.presence_no_interaction)


fit.presence_regdummy <- combined_data_cor_screened_presence |> lmer(formula = presence ~ 1 + age + media + media * ar_frequency +  (1  | exp_title ) + (1 |  id),)

fit.presence_no_interaction_regdummy <- combined_data_cor_screened_presence |> lmer(formula = presence ~ 1 + age + media + ar_frequency +  (1  | exp_title ) + (1 |  id))


# Data: combined_data_cor_screened_presence
# Models:
# fit.presence_no_interaction: presence ~ 1 + age + media + ar_frequency + (1 | exp_title) + (1 | id)
# fit.presence: presence ~ 1 + age + media + media * ar_frequency + (1 | exp_title) + (1 | id)
#                             npar   AIC   BIC  logLik deviance  Chisq Df Pr(>Chisq)
# fit.presence_no_interaction    7 13406 13452 -6695.9    13392                     
# fit.presence                   8 13407 13460 -6695.5    13391 0.8032  1     0.3701

tab_model(fit.presence_regdummy)

palette <- viridis(n = 2)

arfreq_plot <- combined_data_cor_screened_presence |>
  mutate(ar_frequency = factor(ar_frequency)) |>
  ggplot(aes(x=ar_frequency, y=presence)) +
    scale_x_discrete(labels=c("Never", "less than once a month", "About once a month", "Several times a month",  "A few times a week", "About daily"), limits=c("0", "1", "2", "3", "4", "5")) +
  geom_violin(trim=T, fill=palette[1], alpha=.35, ) +
    geom_jitter(alpha=.3, color='black', shape=21, size=1,  fill="white") + 
 stat_summary(fun = "mean", shape=21, size=.5, color='black', fill="black") +
  stat_summary(fun.data=mean_cl_normal, 
                 geom="errorbar", width=.2,  linewidth=1, color='black')+
    labs(x = "In the past 12 months, how frequently have you used AR filters?",
       y = "Presence") +
  theme(text=element_text(size=14),
        axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) + theme_minimal() +
  coord_flip()

media_plot <- combined_data_cor_screened_presence |>
  ggplot(aes(x=media, y=presence)) +
   scale_x_discrete(labels=c("AR", "Flat"), limits=c("ar", "flat")) +
  geom_violin(trim=T, fill=palette[2], alpha=.35 ) +
   geom_jitter(alpha=.3, color='black', shape=21, size=1,  fill="white") + 
 stat_summary(fun = "mean", shape=21, size=.5, color='black', fill="black") +
  stat_summary(fun.data=mean_cl_normal, 
                 geom="errorbar", width=.2,  linewidth=1, color='black', fill='black')+
      labs(x = "Media", y = "Presence") +
  theme(text=element_text(size=14)) + theme_minimal() +
    coord_flip()

graph_combined <- (media_plot / arfreq_plot) + plot_layout(axes  = 'collect', )
  
print(graph_combined)

graph_combined |> ggplot2::ggsave(filename='presence_graph_combined.png',width=19.0, units='cm')

```


enjoy
```{r}

combined_data_cor_screened_presence |>
  mutate(ar_frequency = as.integer(ar_frequency)) |>
  select(enjoy, age,media, ar_frequency) |>
  corrr::correlate() |>
  corrr::rearrange(method = "MDS", absolute = FALSE) %>%
  corrr::shave() %>% 
  corrr::rplot(shape = 19, legend = TRUE, print_cor=T)


# note that we dropped the cohort random factor as the second model failed to converge
fit.enjoy <- combined_data_cor_screened_presence |> lmer(formula = enjoy ~ 1 + age + media* ar_frequency +  (1  | exp_title ) + (1 | id), contrasts = list(media = contr.sum))

fit.enjoy_no_interaction <- combined_data_cor_screened_presence |> lmer(formula = enjoy ~ 1 + age + media + ar_frequency +  (1  | exp_title ) + (1 |   id), contrasts = list(media = contr.sum))

anova(fit.enjoy, fit.enjoy_no_interaction)

# fit.enjoy_no_interaction: enjoy ~ 1 + age + media + ar_frequency + (1 | exp_title) + (1 | id)
# fit.enjoy: enjoy ~ 1 + age + media * ar_frequency + (1 | exp_title) + (1 | id)
#                          npar   AIC   BIC  logLik deviance  Chisq Df Pr(>Chisq)
# fit.enjoy_no_interaction    7 16868 16914 -8426.8    16854                     
# fit.enjoy                   8 16868 16921 -8426.0    16852 1.6202  1     0.2031

tab_model(fit.enjoy_no_interaction)

palette <- viridis(n = 2)

arfreq_plot <- combined_data_cor_screened_presence |>
  mutate(ar_frequency = factor(ar_frequency)) |>
  ggplot(aes(x=ar_frequency, y=enjoy)) +
    scale_x_discrete(labels=c("Never", "less than once a month", "About once a month", "Several times a month",  "A few times a week", "About daily"), limits=c("0", "1", "2", "3", "4", "5")) +
  geom_violin(trim=T, fill=palette[1], alpha=.35, adjust=3  ) +
    geom_jitter(alpha=.3, color='black', shape=21, size=1,  fill="white") + 
 stat_summary(fun = "mean", shape=21, size=.5, color='black', fill="black") +
  stat_summary(fun.data=mean_cl_normal, 
                 geom="errorbar", width=.2,  linewidth=1, color='black')+
    labs(x = "In the past 12 months, how frequently have you used AR filters?",
       y = "Enjoyment") +
  theme(text=element_text(size=14),
        axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) + theme_minimal() +
  coord_flip()

media_plot <- combined_data_cor_screened_presence |>
  ggplot(aes(x=media, y=enjoy)) +
   scale_x_discrete(labels=c("AR", "Flat"), limits=c("ar", "flat")) +
  geom_violin(trim=T, fill=palette[2], alpha=.35, adjust=3 ) +
   geom_jitter(alpha=.3, color='black', shape=21, size=1,  fill="white") + 
 stat_summary(fun = "mean", shape=21, size=.5, color='black', fill="black") +
  stat_summary(fun.data=mean_cl_normal, 
                 geom="errorbar", width=.2,  linewidth=1, color='black', fill='black')+
      labs(x = "Media", y = "Enjoyment") +
  theme(text=element_text(size=14)) + theme_minimal() +
    coord_flip()

graph_combined <- (media_plot / arfreq_plot) + plot_layout(axes  = 'collect', )
  
print(graph_combined)

graph_combined |> ggplot2::ggsave(filename='enjoy_graph_combined.png',width=19.0, units='cm')

```

